## 1. 학습 날짜
+ 2020-12-21(월)

## 2. 학습시간
+ 13:00 ~ 19:00 (자가)   
+ 20:00 ~ 24:00 (자가)
+ 총 학습시간 : 10시간

## 3. 학습 범위 및 주제
+ 컴퓨터구조

## 4. 동료 학습 방법
+ 대학 동기와 질의 응답

## 5. 학습 목표
+ Multithreading(SMT) 학습
+ Memory 학습

## 6. 상세 학습 내용
+ 실제 코딩에 소요한 시간 : 0시간    
+ 컴퓨터구조 강의 수강 : 3시간    

### SMT   
   
3. Simultaneous Multithreading   
동시에 멀티쓰레딩 한다는 것이 핵심   
FG/CG MT 도 한 쓰레드 내에 여러 명령어가 처리되는 경우, 한 stage 안에서도 data dependency, control hazard 발생할 수 있다. 그러면 버블이 생길 수 있다. 문제가 된다.    
그래서, 이 문제를 해결하고 활용도를 극대화하기 위해 명령어 처리개수를 엄청나게 늘려보자   
다중의 thread의 다중의 명령어를 1 clock cycle 안에서(매클락마다) 뽑아와서 처리하자.   
   
멀티-쓰레드 -> 한 프로세서 안에서 분리. 1개의 cpu에서 다중의 작업.   
멀티-프로세서 -> 듀얼코어, 쿼드코어(물리적 배속) 쓰면서 작업을 분리.   
   
FG/CG -> 명령어를 1 clock에서 1개의 thread에서 가져옴   
SMT -> 명령어를 1 clock에서 다중의 thread에서 가져옴   
   
빨간색 명령어   
흰색 버블   
색깔 다른 것은 다른 쓰레드를 의미함.   
   
멀티쓰레드는 기본적으로 싱글 쓰레드보다 system-wise 성능이 극대화.   
SMT 에서 버블이 거의 사라짐.   
그리고 추가적으로 한 클락에서 우선순위가 높은 쓰레드부터 채울 수 있다.   
즉, 싱글 쓰레드의 성능도 거의 동일하게 맞춰줄 수 있다.(빨간색이 우선순위가 높다한다면, 빨간색으로 다 채우면 됨)   
thread-wise 성능도 증가, system-wise 성능도 극대화.   
허점이 거의 없다.   
문제는 구현의 어려움(data dependency, control hazard checking, multithreading)   
AMD 성공의 요인 : SMT   
   
>horizontal vs Verrical   
파이프라인을 옆으로 누인것   
horizontal waste    
verrical waste   
중요하지 않다.   
   
>smt   
성능이 극대화(수평.수직 waste를 감소)   
많은 양의 하드웨어 요구(명령어를 가져와도 되는지 분석하기 위해)   
   
>smt pipeline   
명령어를 register mapping : 쓰레드마다 각각의 레지스터 데이터를 가지고 여기에 명령어를 매핑   
Queue : 명령어를 queue에 넣는다.(loop unrolling)   
for문에서 loop unrolling 하면 cpu(HW입장)는 훨씬 빠르다.(컴파일러는 loop unrolling 하면 exe크기가 굉장히 커짐(SW입장))   
   
Reg read : register file을 read   
Register Map : 명령어가 들어왔을 때, 실제 피지컬한 레지스터 주소와 맞춰주는 역할을 함.   
왜 하냐면, 옛날 cpu에서 프로그램된 것을 지금 cpu에서 돌아가야 한다. 그것을 위해 매핑한다.   
   
Execute : 명령어를 다중처리할 수 있도록, 즉 수퍼스케일라   
   
Retire : OoO이면, retire할 때도 순서가 제대로 되었고, 문제가 없었는지 확인해야 함.   
그것을 score board라고 함.   
무슨 명령어가 어떻게 처리되었는지 정보를 가지고 있다가 끝나면 날려버린다.   
앞의 작업에서 문제가 없었는지를 확인하는 단계 (성능에 영향을 주지는 않는다)   
   
SMT는 모두 뒤죽박죽이지만, SW 엔지니어 입장에서는 뒤죽박죽처럼 보이지 않아야 한다.   
그래서 HW 구현이 어렵다   
   
그래서 SMT를 하기 위해 필요한 파이프라인?   
- replicated resource   
쓰레드별로 자원이 필요. 다중 자원   
   
- shared resource   
register file은 share 할 수도 있고 share 안 하고 복제할 수도 있는데, share 한다면 사이즈를 늘려서 쓰면 된다.   
장점 : 쓰레드마다 사용하는 레지스터 파일의 개수가 다를 경우, 어떤 쓰레드는 레지스터를 몇개 안쓰는데, 어떤 쓰레드는 레지스터를 많이 사용함.   
이 경우, 레지스터를 share하여 레지스터를 많이 필요로 하는 쓰레드가 많이 사용한다면 성능이 좋아진다.   
instruction queue : 공유해야 함. 큐는 하나   
caches : 캐시도 공유해야 함. 캐시를 따로 하면 하드웨어가 많이 필요해진다.   
캐시는 비쌈. 캐시가 cpu의 2/3 차지. cpu+cache가 반도체 면적의 1/3이 차지.    
이렇게 비싼데, 복제할 수가 없다.   
translation lookaside buffer(TLB): 나중에 배움, SMT와 TLB는 굉장히 중요   
branch predictor : 공유할수도 있고, 안 할수도 있다.   
스레드별로 달리 만들어도 된다. 그러면 확률이 좋아진다. 브랜치 예측은 확률이 굉장히 중요하다.   
브랜치 예측도 하드웨어 사이즈가 꽤 크다.   
   
녹색은 공유할 수 도 있고, 개별적으로 둘 수 도 있다,  설계자가 많은 실험 데이터로부터 근거를 만들어 결정   
   
- Changes to OoO+Super Scalar for SMT.   
OoO와 SS를 하기 위한 변화, 여기에 SMT를 붙이면 극강.    
여러개의 pc중에서 하나를 선택하는 메카니즘이 필요하다.   
per-thread stack 필요. 쓰레드마다 context가 다르기 때문   
쓰레드별로 명령어의 흐름을 처리할 수 있는 메카니즘이 필요하다.   
쓰레드 id별로 브랜치 타겟 버퍼를 만들어야 한다.    
큰 레지스터 파일이 필요하다.    
   
보통 논문 발표 5년 이후 상용화가 된다.   
   
- SMT Scalability   
한 사이클에 4개(max)의 명령어를 가져올 수 있는데, 싱글 쓰레드일 경우 평균적으로 2.1개 정도 가져온다.   
쓰레드 증가할 수록 4개에 근접해지고, bound된다.   
쓰레드 개수를 늘릴때마다 cpu HW size는 꽤 많이 증가한다. 쓰레드별로 검사해주는 하드웨어의 증가(n배 증가)   
6개 이상으로 쓰레드를 더 늘리면 HW가 커지는데, 성능은 4배로 비슷하다.   
보통 쓰레드 3~4개로 선택한다.(=> HW를 고려한 최적의 개수를 결정한다, 실험을 통해 효율성을 결정)   
   
---------------------------------------------------------------   
   
- SMT design consideration   
1. 매클락마다 어느 쓰레드에서 명령어를 가져오는데 우선순위를 어떻게 할 것인가.   
   
2. 공유 자원 할당 정책   
- 어떻게 starvation(처리할 명령어가 없어서 파이프라인이 놀고 있는 것)을 방지할지   
- 처리율을 최대화할 수 있는지   
- 어느 쓰레드에서 명령어를 가져오는 것이 공평한지, 전체 쓰레드 처리의 Quality를 어떻게 유지할지   
- 모두한테 하드웨어 자원을 막 줄 것인지, 구분되어 줄건지   
   
   
- Which Thread to Fetch From? / SMT Fetch Policies(1)   
static : 정적인. 하나로 고정한 체계   
 - round-robin : 매 클락마다 여러 쓰레드에서 하나씩 받아오자 단순.   
   
dynamic    
 - minimal in-flight branch : 브랜치가 적은 쓰레드를 선호(브랜치 예측도 해야 하고, 브랜치 많으면 flush 문제가 많다)    
 - minimal outstanding misses : 캐시미스가 적은 스레드를 선호   
(캐시 미스 : 빠른 동작을 하는 캐시한테 명령어를 달라고 했는데, 캐시에 없어.   
그러면 ddr 메모리로 가야하는데 메모리는 200cycle정도를 손해본다. 그동안 cpu는 기다려야 한다.)   
-> cpu 파이프라인 stage fill rate을 최대화 하겠다.   
 - minimal in-flight instructions : 명령어의 수가 적은 쓰레드를 선호(쓰레드마다 명령어의 개수가 다르다)   
 - higher real time requirments : 실시간 연산성을 요청하는 쓰레드 (이메일 확인여부보다 사용자가 현재 사용중인 마우스 반응이 더 중요),   
사용자가 현재 사용중인 쓰레드를 선호. hard real-time 쓰레드를 더 선호해야 한다.   
soft real-time는 웬만하면 0.3초 맞춰줘야 하는 경우.   
hard real-time 무조건 0.3초 맞춰야 하는 경우. 이는 통신의 경우인데, 송신이 된지 0.3초 안에 받아야 수신해야 한다. 수신하지 않으면 신호를 놓치게 된다.   
hard real-time 은 발생가능한 많은 경우를 모두 고려해야 하기 때문에 구현하기 쉽지 않다.    
   
 => round-robin은 좋지 않다. 느린 쓰레드가 파이프라인을 점유하게 되어 전체 성능이 느려지게 된다. monopoly(매점매석)   
   
- SMT Fetch Policies(2)   
Icount : ealry stage에서 명령어의 개수가 적은 쓰레드(=기다리는 명령어가 적은 쓰레드, 빠른 쓰레드)를 가져온다. 빠르게 처리되는 스레드를 빨리 처리해주자.   
   
RR 2.8 : round-robin에서 명령어를 한번에 8개를 가져올 수 있는 cpu에서 스레드당 2개씩 가져온다. 이게 가장 성능이 좋다(그래프상에서)   
좌측 그림에서 RR2.8이 가장 성능이 좋다.   
우측 그림에서 RR2.8보다도 Icount 2.8 이 가장 성능이 좋다   
--> SMT 안에서도 fetch policy를 바꾸니까 성능이 좋다.   
   
논문 : people.csail.mit.edu/emer/papers/1996.05.isca.smt.pdf   
(논문11page Icount부분)   
   
Icount : 파이프라인 앞단에 있는 stage(decode, rename, IQ)에서 명령어를 가장 적게 가지고 있는 쓰레드를 favor 하자; 3가지 purpose(목적)   
(1) IQ(instructuon queue,전체 stage중 3번재 stage): 한 쓰레드가 IQ를 점유해버리는 일을 막는다.(page15, single thread superscalar 하는 것을 막는다)   
renaming : data dependency가 발생하는 것을 방지하기 위해 진행.   
(2) IQ를 가장 효과적으로 지나치는(passing하는) 스레드를 선호. (dependency issue가 없어 빠르게 처리되는 스레드들)   
그런 쓰레드들은 빈공간을 잘 채워준다. 이 스레드들은 시스템 성능을 높여준다.    
즉, icount로 명령어가 적은 스레드가 IQ에 들어오고, IQ에 들어온 명령어가 빨리 처리될수록 system-wise성능이 증가한다.   
(3) 처리가능한 스레드들로부터의 더 합리적인 명령어 분배가 될 것이다. queue에서의 병렬처리는 최대화하면서.   
빨리 처리하는 명령어를 더 빨리 집어넣게 해주니까 병렬처리면에서도 더 좋은 것이다.   
   
queue에서 OoO를 한다. 순서를 바꾼다.   
fetch/decode/map는 명령어를 처리하기 위한 준비단계임   
그 이후에 명령어를 처리한다.   
   
여러 정책을 만들어 놓고, 실험을 통해 가장 효율적인 정책을 찾아낸 것임.   
   
인텔은 지금도 hyper-threading 하고 있다.   
hyperthreading : 요즘 cpu는 보통 threading이 core의 두배. SMT를 해준다.   
요즘 상용 cpu는 supersclar를 2개로 해준다(효율적이니까).   
hyper-threading을 안하는 cpu는 core개수와 threading 개수가 같다. 즉, SMT를 안하는 것이다.   
hyper-threading은 SMT를 하는지 안하는지 여부.   
   
(page 33) : 15stage   
요즘은 아마 20stage까지 한다.    
trace cache : 원래 캐시는 최근에 썼던 명령어를 다시 쓰거나, 근처에 있는 명령어을 처리하는 것인데,   
trace cache는 최근에 처리한 순서대로 명령어를 쌓아 놓는 것임. trace cache는 최근에 브랜치한 패턴을 저장해놓는다. 비선형적인 읽기순서를 저장하고 있다.   
     
hyper-threading 을 하기 때문에 rename이 필요하다.(수퍼스케일라니까)   
sched(schedule) 까지가 준비단계   
실제로 쓰는 기술이다.   
   
상용 cpu, server cpu, AMD cpu도 보통 2개 쓰레드를 한다.   
그 이상은 하드웨어 구현이 너무 어려워진다. 효율적이지 않기 때문.   
스레드 2개만 돌려줘도 성능이 많이 좋아진다.   
   
-----------------------------------------------------------------------------------   
   
### Memory Hierarchy  
   
CSE는 0과 1의 세계(논리적 해석)   
EE는 몇 V,A인지를 판단(물리적 해석)   
   
> Idealism (이상향) : 현실은 그렇지 못하다.   
   
Pipeline(CPU입장에서 원하는 사항)   
- No pipeline stalls : address 주는 순간 데이터를 바로 뽑아주기를 원한다. stall이 없으려면 속도가 굉장히 빨라야 하는데 그렇지 못한다.   
- Perfect data flow (reg/memory dependencies) : 디펜던시가 없었으면 좋겠다.   
- Zero-cycle interconnect (operand communication) : wire(선)들도 딜레이가 없었으면 좋겠다.   
- Enough functional units : 하바드구조. 분리해서 좋은점은 structure hazard 없다. 가격이 저렴해서 여러개 샀으면 좋겠다.   
- Zero latency compute : 지연도 없었으면 좋겠다.   
   
Instruction Supply(Memory 입장)   
- Zero-cycle latency(time)   
- Infinite capacity   
- Zero cost : 저렴한 가격   
- Perfect control flow   
   
Data Supply   
- Zero-cycle latency   
- Infinite capacity   
- Infinite bandwidth : 명령어 하나가 무지하게 큰 데이터를 요구할 수 있는데, 그 떄 한 클락만에 전달하고 싶다.   
- Zero cost   
   
> The Memory Hierarchy(메모리 하이라키(계층))   
성능의 진짜 핵심은 메모리를 어떻게 다루느냐. 굉장히 중요! 왜냐면 메모리가 굉장히 느리기 떄문.   
   
L1 cache는 core 안에 있다. data와 instruction이 2개의 캐시로 분리되어 있다.   
L2 cache는 각 core 옆에 하나씩 붙어있다. data와 instruction이 하나로 뭉쳐있다.   
L3 cache는 4개의 코어가 공유한다. 크다.   
DRAM은 L3크기만한 chip 여러개로 묶여있다. 물리적인 크기 차이가 크다.   
   
Higher bandwidth is more expensive   
- Need more banks(메모리를 구성하는 묶음), more ports(핀이 많아서 한번에 많이 보내주든지),   
higher frequency(단위시간당 많이 보내준다), or faster technology(미세공정)   
   
이상과 현실은 다르다   
   
> DRAM   
- memory cell   
으 모양 : trasistor중 gate   
gate가 0(논리적 0)이 되면 연결(PMOS(metal oxide Semiconductor)에서)   
gate가 1(논리적 1)이 되면 끊김   
스위치 역할. 반대는 NMOS   
   
캐패시터 : 전자를 저장. 쌓는다.   
write : gate 0이라 연결되고, 전류를 흘러보내면(1을 쓴다) 전자가 캐패시터에 쌓인다. 쌓이면 gate를 1로 하여 연결을 끊는다. 0을 쓰면 캐패시터의 전자가 나온다.   
read : gate 0으로 하면 연결되어, 캐패시터에 쌓인 전자가 밖으로 흘러나오고 sense amplifier로 증폭되어 1인지 0인지를 읽어낸다.   
(sense amplifier : 전자가 조금 흘러나오면 증폭시켜 데이터를 무엇인지 빨리 알아내는 회로)   
    
- DRAM은 왜 dynamic인가?    
캐패시터의 전압이 시간이 흐르면서 감소한다. data가 날라가게 된다.(누설전류가 생겨 전압을 잃는다. CS입장에서는 시간이 흐르면 데이터가 뒤집힌다,)   
data를 날라가지 않도록 주기적으로 refresh를 해준다. DRAM는 보통 10ms마다 해준다.   
10초정도 refresh를 하지 않으면 데이터가 날라간다.   
데이터가 시간 지나면 동적으로 날라간다.   
   
데이터가 날라가니까 안 좋은데 왜 사용?   
트랜지스터와 캐패시터의 크기가 비슷. 트랜지스터 위에 캐패시터를 쌓아서 물리적인 위치상 하나의 자리만큼만 차지한다.   
-> 가격이 저렴하다는 장점!   
SRAM은 static : refresh를 안해도 데이터가 안 날라간다. 그러나 트랜지스터를 많이 사용해서 공간을 많이 차지하여 비싸다는 단점!   
   
> SRAM   
write : gate가 0이면 연결되어 10배 이상의 강한 전류가 흘러(인버터에 흐르는 전류에 비해) 데이터를 쓴다.   
gate 1이면 연결이 끊겨, 인버터 두개 사이의 전류가 흘러 데이터를 가지고 있다.     
즉, data refresh를 안해도 데이터가 날라가지 않는다.   
read : gate 0이 되어 연결되면, 인버터로부터 전류가 흐르고, sense amplifier로 증폭시켜 읽는다.   
invertor에 트랜지스터 2개가 들어간다. 총 트랜지터 6개 들어간다. 면적으로 따지면 DRAM보다 6배 차이가 난다.   
   
> Memory Bank Organization and Operation   
row 선택하고 col 선택한다.   
row를 여러 개로 많이 나누고, col의 개수를 적게 하여 전력 소모를 줄일 수 있다.   
한 row에 col이 많으면 전력 소모가 증가하기 때문.   
-> Memory Bank구조   
bank가 또 여러개 존재 -> bank enable이 생겨서 특정 bank를 선택한다.   
bank enable : 어떤 bank를 가려고 하는지   
bank도 커지면 속도가 느려진다.   
실험을 통해 효과적인 bank size/개수를 결정하여 제품을 생산한다.   
   
Amplify : 시간을 줄이기 위해 증폭시킨다. 조그만한 신호가 나오면 증폭시킨다.   
   
SRAM도 마찬가지로 bank 구조 사용한다.   
   
> DRAM vs. SRAM   
   
DRAM : 수동소자, main memory에 사용된다.   
 - Slower access (capacitor)    
 - Higher density (1T 1C cell) : 집적률이 높다.(4~6배 정도까지). bank size가 크다.   
	MUX 단위가 다르다. MUX의 depth(layer)가 달라진다(하드웨어 size가 증가),   
	즉, 데이터 뽑는 속도가 느려진다. 한꺼번에 많은 데이터를 뽑을 수 있기 때문.   
 - Lower cost : 싸다. 왜냐면 공간을 적게 차지하기 때문. 1bit를 저장하기 위해 필요한 단위면적이 작다.   
 - Requires refresh (power, performance, circuitry)   
 - Manufacturing requires putting capacitor and logic together   
   
SRAM :인버터는 능동소자. cache에 사용된다. 캐시는 DRAM에 비해 굉장히 빠르다.   
 - Faster access (no capacitor)   
 - Lower density (6T cell)   
 - Higher cost   
 - No need for refresh : refresh 안 하여 속도가 빨라진다.   
 - Manufacturing compatible with logic process (no capacitor)   
   
> The Problem   
- Bigger is slower   
sub-nanosec : Shorter in duration than a nanosecond. 0.25nanosec   
L1 cache 와 Cpu는 sub-nanosec이라서 연동이 가능하다. 대신 L1캐시의 사이즈가 512byte로 작다   
   
L1과 DRAM의 차이   
4gHz는 1클락이 0.25nanosec -> 50nanosec의 1/200. 즉 데이터 읽을 때 200cycle정도의 차이가 발생한다. 그래서 메모리 읽을때 200cycle정도 차이가 난다고 한것이다.   
(요즘은 더 캐시의 속도와 메모리 속도가 빨라지긴 했지만 비율은 비슷하다.)    
   
5GHz까지는 소자와 기술의 발전으로 가능했지만, 그 이상은 발열때매 힘들다. (상용 pc에서)   
온도를 영하로 낮추고 클락수를 높여 성능을 높이는 연구 -> 과학적인 연구 목적으로..    
   
부팅 속도가 느린 이유 : 하드디스크로부터 메모리로 올리는 속도가 느리기 때문이다.   
   
요즘은 hard disk < 0.1달러 per Gb   
무어의 법칙 : 하드웨어는 2년마다 2배씩 증가한다. 가격이 반으로 내려간다.   
10년 지나면 2^5배   
이런 가격들은 시간이 지남에 따라 scale 된다. (무어의 법칙에 비례해서 scale)   
무어의 법칙이 왜 가능한가? : 반도체 공정의 발전으로 가능함.   
30% 감소된다.(면적은 제곱이라 0.5감소) 새로운 공정이 나오는데 2년이 안 걸린다.   
10나노 공정 : 10*10=100   
7나노 공정(30%감소) : 7*7=49   
-> 10나노에서 7나노로 바뀌면 면적은 50% 감소한다.    
   
-----------------------------------------------   
> Why Memory Hierarchy?   
여기서부터 본론.   
프로세서가 필요로 하는 데이터를 빠른 레벨에 유지시키는 것이 중요   
-> 목표하는 fast + large를 이룰 수 있다.   
   
> Locality ***** : 지역 한정성   
- Temporal Locality : 시간적인 데이터의 지역한정성. 높은 확률로 한 일을 다시 할 것이다. 변수 초기화 이후 그 변수를 다시 사용할 확률이 크다.   
- Spatial Locality : 변수 a를 요구하면, b,c,d도 같이 준다. a를 쓰는 순간 b,c,d를 쓸 확률이 높을 것이다.    
--> 컴퓨터가 동작하는 것을 관측하면서 발견하였다.   
-> 이것을 잘 활용해서 캐시를 만들면 성능이 좋아질 것이다.   
   
DRAM은 덤프, cache는 스포츠카   
DRAM는 느리지만 한번에 많은 데이터를 줄 수 있다.   
   
cache block : 한 블락이 자기가 받은 것을 다 쌓아놓는 것 . 32bit 시스템에서 4바이트가 명령어 한개   
64byte block은 8개 명령어를 한 라인에 다 쌓는 것이다.   
   
> Caching in a Pipelined Design   
L1 cache가 하드웨어 커지면 속도가 느려지는데, 어느순간부터 cpu 보다 느려지면 연동이 불가능하다.    
물론 L1이 크면 클수록 좋지만(더 많은 데이터를 넣을 수 있어 DRAM까지 갈 필요가 없기 때문), 불가능하다. cpu와 같은 속도가 필요하다.   
   
L2 : L1보다는 4배 정도 느리다, 관찰 및 분석하여 최적의 size를 결정하였다.    
   
캐시에 새로운 데이터를 넣으려면, 가장 오래전에 사용한 데이터를 지워야 한다.   
L1에서 데이터를 못찾으면 L2에서 찾는다. DRAM에서 찾는 것보다는 패널티가 적다.   
   
쓰레드 : 손 2개면 동시에 2개 책을 펼쳐서 볼 수 있다는 장점.    
   
>A Note on Manual vs. Automatic Management    
cache management   
- Manual : 소프트웨어로 데이터를 어디에 놓을지 결정   
(CUDA programming : 캐시와 다르게 사용자가 data를 어디에 올려놓는가를 결정할 수 있다. 짜는 사람에 따라 성능이 제각각이다.   
GPU에서 프로그래밍하는 것. 알고리즘 잘하면 추천. 캐시와는 다른 의미. 전문가적 지식이 필요하다.   
소프트웨어 엔지니어의 하드웨어 감각이 얼마나 좋은지에 따라 성능이 달라진다.)   
- Automatic : 하드웨어가 알어서 데이터 이동을 관리한다. 캐시는 하드웨어.   
하드웨어가 하면 순식간에 가능한데 이것을 소프트웨어가 한다고 하면 속도가 느려진다. 자동화해서 돌려주는 것.   
   
캐시를 쓰는 이유는 locality 때문.   
   
32KB = 8000개의 명령어 (4byte에 명령어 1개)   
   
캐시에 데이터가 존재하면 hit, 없으면 miss   
캐시는 태그라는 것이 붙어 같은 데이터라도 메모리보다 하드웨어를 많이 차지한다.   
메모리 계층이 없다면, cpu의 4GHZ가 무의미하다.   
    

## 7. 학습 내용에 대한 개인적인 총평
+ SMT까지 학습하면서 cpu에 대한 전체적인 범위를 다루었다.
+ Memory Hierarchy을 사용하는 이유를 중점으로 학습하였다.
+ pipeline hazard부터 시작해서 마지막 SMT까지의 cpu 변화의 역사의 흐름을 이해하며 학습하였다.
+ DRAM과 SRAM에 대한 내용을 복습하였다.
+ 내일은 마지막 cache와 VM에 대한 내용을 학습할 예정이다.

## 8. 다음 학습 계획
+ 컴퓨터구조 Cache, Virtual memory 학습