## 1. 학습 날짜
+ 2020-12-22(화)

## 2. 학습시간
+ 14:00 ~ 18:00 (자가)   
+ 20:00 ~ 24:00 (자가)
+ 총 학습시간 : 8시간

## 3. 학습 범위 및 주제
+ 컴퓨터구조

## 4. 동료 학습 방법
+ 대학 동기와 질의 응답

## 5. 학습 목표
+ Cache 학습

## 6. 상세 학습 내용
+ 실제 코딩에 소요한 시간 : 0시간    
+ 컴퓨터구조 강의 수강 : 3시간    

### Cache   
   
캐시는 SRAM으로 만들어진다. SRAM은 휘발성.    
수십년간 사용되온 SRAM을 대체할 비휘발성의 고속의 저전력의 값싼 메모리의 등장을 기대한다.   
DRAM,SRAM이 비휘발성이 되면, 부팅에 필요한 데이터를 메모리에 부팅할때마다 올릴필요가 없어진다. 메모리에 저장되어 있기 때문에.   
그러면 다시 SSD에서 HDD로 갈 수도 있을 것 같다.   
SSD는 부팅 속도와 디스크 간 큰 데이터를 주고받을 때 속도가 빠른 것인데,   
큰 용량의 HDD 사용하면 큰 데이터를 주고 받을 일도 없어지기 때문에 비싼 SSD를 살 이유가 없어진다. 그런 시대가 올 수도 있다.   
   
PROCESSRO 1ns(1ghz이상은 하니까)   
DRAM은 수백에서 50ns까지 느려진다.   
processor와 DRAM간의 속도 차이(GAP)을 줄이는 것이 관건.   
150 cycle정도의 차이가 있다.   
이게 프로세스입장에서 가장 큰 문제이다.   
캐시가 이 문제를 해결한다.   
   
- 9.3 Basic terminologies 용어 암기   
   
hit : 찾고자 하는 데이터를 캐시에서 찾았다.   
hit rate : 해당 캐시에서 hit할 확률   
miss penalty : 못찾았을 때의 패널티, 즉 cpu가 노는 것    
EMATc: Tc(c번째 캐시에 hit이 났을 때 엑세스하는 타임) + m(Miss rate) * Tm(그 다음 단계에서 엑세스 타임).   
c단계 캐시는 hit이든 miss든 Tc 동안 c단계 캐시를 무조건 엑세스한다.   
miss시 다음 단계로 계속 nesting.    
HDD는 10ms까지 느리다. cpu입장에서는 굉장히 느린 시간.   
   
캐시가 작을수록 속도가 빠르다.   
   
- 9.5 Cache organization   
1. Placement : 데이터를 위치하는 것   
2. Algorithm for lookup : 모든 데이터를 읽지 않기 때문에, 찾는 데이터가 있는지 없는지를 찾는 것이 중요하다.   
3. Validity : 찾은 데이터가 유효한지   
   
- 9.6 Direct-mapped cache organization   
갈 수 있는 위치가 정해져있다.   
plcement   
뒤의(하위) 세자리가 일치하는 위치로 갈 수 있다.   
Cache_Index = Memory_Address mod Cache_Size   
Cache_Tag = Memory_Address / Cache_Size   
앞의 두자리는 엔트리의 태그에 저장한다   
   
실제 메모리는 주소에 가면 데이터만 있으나.   
캐시는 태그와 데이터(content)가 있다.   
   
인덱스를 따라가서, 태그가 같으면 해당 엔트리의 content를 준다.   
태그가 다르면, 데이터가 없는 것이고, 메모리의 다음 레이어로 이동한다.   
   
power를 껐다가 다시 켰을 경우? 랜덤값으로 비트가 정해진다.    
실제로는 F가 많은데 현재 0으로 다 정해졌다고 가정.   
그리고 아직 데이터를 메모리로 안 가져왔다고 가정   
그런데, 우연히 태그가 맞아서 갔는데, cpu가 잘못된 데이터를 가져가서 cpu의 계산이 엉뚱해진다. 연산에 오류가 발생!   
이것을 막을라고 valid bit가 생겼다.   
   
valid를 0으로 설정하면, 데이터가 없다는 의미이다.   
   
메모리를 강제로 어떤 값으로 초기화 할 수는 있으나, 메모리 측면에서 하드웨어가 늘어난다.   
그러나 랜덤으로 나몰라라하면, 하드웨어는 적게 들지만, 계산에 오류가 발생한다.   
   
따라서, power를 껐다가 다시 키면, valid는 모두 0으로 만들고, 나머지는 랜덤으로 정해진다.   
우연히 태그가 맞아도, valid가 0이면, 캐시에 데이터가 없다고 판단할 수 있다.   
메모리에서 데이터를 업데이트하면 그 순간 valid 를 1로 만든다.   
캐시도 하나의 sub-system으로 봐야 한다.   
   
- 9.7 Repercussion on pipelined processor design   
Miss on I-Cache: Insert bubbles until contents supplied, 명령어가 오기전까지 파이프라인에 버블을 집어놓는다   
Miss on D-Cache: Insert bubbles into WB stall IF, ID/RR, EXEC, miss가 발생하면 버블을 5 stage에 넣는다.   
   
- 9.8 Basic cache read/write algorithms    
Read Hit : 데이터가 캐시에 존재할 때    
Read Miss ; 데이터가 캐시에 없다. 메모리 주소가 system bus로 가서 메모리의 데이터가 system bus로 cpu에 전달된다.(약 150cycle동안)   
또한 data가 cache에 써진다. valid가 1로 바뀌고, tag, data 정보가 바뀐다.   
   
Write-Back : 캐시에는 무조건 써야 한다. 그래야 다시 읽을 수 있기 때문에. cpu register에 공간이 모자르면, 최소 못해도 cache에 써야 한다.   
a를 갱신하여 저장할 때 new a를 메모리에 저장하였다면, 다시 a를 읽어올 때 cache에 있었던 old a를 가져오기 때문에, 문제가 발생한다.     
cache에만 쓰는 것이 Write-back   
cpu는 다른 일하고 있고, cache는 system bus가 한가할 때, 메모리에 데이터를 쓴다.    
   
Dirty bit : 1이면 dirty. dirty가 1이라는 것은 캐시가 나만 가지고 있다는 표현.   
그러면 system bus를 주시하고 있다가 한가하면 데이터를 보낸다.   
Write-Through의 write buffer는 데이터를 바로 메모리에 쓰려고 하기 때문에 congestion을 많이 발생시킨다.   
그러나 Write-back은 dirty bit로 system bus를 엿보고 있기 때문에 system utilization이 더좋다. (대신에 빨리 못 쓴다는 게 단점)   
write-back은 multi-core일 때 문제가 된다.   
데이터가 항상 메모리에 업데이트 된다고 하면, cpu들이 메모리만 주시하면 되는데,   
write-back해버리면, 한 cpu가 독자적으로 데이터를 가지고 있기 때문에 문제가 있다.   
하드웨어 구현이 더 복잡해진다. 그러한 단점이 있다.   
   
Write-Through : cache에도 쓰고(1clock~수십 clock), 메모리에도 쓴다(150cycle)   
동시에 데이터를 쓰기 때문에 그동안 cpu가 놀아야 한다.   
데이터는 바로 저장해야 해서 맘 편하기는 하지만, memory에 write 될 때까지 놀아야 한다.   
Write-through는 cpu가 노는 시간이 길어진다.   
장점은 구현이 쉽다.    
write buffer를 만들어 cpu는 다른일을 하고, 버퍼에 담긴 데이터를 메모리에 write한다.   
->cpu가 노는 시간을 조금 줄일 수 있는 효과가 있다.   
   
결국 Write-back과 Write-Through를 혼용해서 사용한다. -> Why?   
   
   
------------------------------------------------------------------------   
9.8.2.3 Comparison of the Write Policies   
   
system bus에는 모든 데이터가 돌아다닌다. 지나가는 데이터가 cpu보다 훨씬 많을 수 있다.   
그런 상태를 생각해보면 write-back/through policy를 고려해볼 필요가 있다.   
write-through에서 cpu가 자기가 필요한 데이터 소량을 wirte하려고 bus를 간섭해버리면, system bus 내의 다른 데이터를 간섭하는 꼴이 된다.   
아무리 write buffer를 넣은들 하더라도, cpu가 blocking 될 소지가 있다. write-buffer가 가득 차면, cpu가 기다려야 하는 문제.   
그에 반면, write back은 cpu입장에서 나몰라라임. 그럴 문제가 아예 없는 것임   
   
write through의 장점은 버퍼를 만들면 되기 때문에 로직이 단순하고 빠르다. bus를 주시해야 하는 로직보다는 단순하다.   
그러나 high traffic을 bus에 만들고, 메모리 엑세스를 어거지로 하다보니 traffic이 많아져서 cpu가 blocking될 수 있고, 다른 시스템 내의 하드웨어 동작이 간섭받을 수 있다.   
   
write back은 로직이 추가되지만, 메모리 엑세스의 효율이 높아진다.   
   
DRAM은 기본적으로 randon access이지만, burst read mode라고 sequential access하여 속도를 높이는 모드가 있다.   
sequential에서 random으로 바뀌면, latency가 느려지고, 효율성이 낮아진다.    
spatial locality : a가져오면 근처의 data도 가져오는 방법. 이는 cache의 hit rate도 높이는 장점이 있고, 또 하나의 장점은 DRAM이 sequential access가 되기 때문에 , burst mode가 가능해진다. 그래서 random access보다 속도가 빨라진다. 이렇게 합이 맞아서 시스템의 효율을 올려준다.   
이렇게 자연적으로 아이디어가 맞아들어가면 좋은 연구가 되는 것이다. 안되면 부가적으로 구현이 필요한 것이다.   
   
**Multilevel cache processors may use both**   
– L1 Write through : dirty bit을 자연적으로 가질 수 없다. -->???   
– L2/L3 Write back : L2부터는 상대적으로 L1에 비해 시간이 오래 걸리니까, dirty bit를 가지고 남는 시간 동안 메모리에 쓴다. 메모리 엑세스 효율성이 높아진다.   
하부 구조로의 dirty bit을 가지겠다는 것은 그 레벨의 메모리 엑세스 효율을 높이겠다는 것이고, dirty bit을 안 가지게 되면 시간차가 생기면서 비효율이 생기게 된다.   
   
- 9.9.1 Improving cache performance   
miss rate와 miss penalty를 낮춰야 한다.(강의자료 오타)   
data placement algorithm을 똑똑하게 만들어서, 웬만하면 hit이 나도록 한다.   
cache size를 늘려서(data가 더 많이 존재한다) 자연스레 hit rate을 늘려야 한다.    
근데, miss가 난다고 하더라도 miss penalty를 줄여야 한다.   
   
L1 L2 L3를 만들었다는 것은 패널티를 최소화하기 위함이다. miss 되었을 때 DRAM으로 바로 가지 않게끔.   
   
- 9.10 Exploiting spatial locality to improve cache performance   
근처에 있는 word까지도 한번에 다 읽어온다.   
캐시는 multiple word를 같이 저장한다.   
cache block은 cache line이라고도 불린다.   
캐시 라인의 길이도 시스템마다 정할 수 있는데, 벤치마크 테스트를 통해서 최적의 라인길이를 결정한다.   
target clock speed가 있으므로, 캐시의 전체 크기는 한정되어 있다. 그 안에서 라인의 크기와 개수는 조절할 수 있다.   
너무 라인의 길이를 늘려서 라인의 개수를 줄이면, hit rate을 낮춘다. 너무 과도하게 라인길이를 늘리면.   
너무 라인의 길이를 줄여서 라인의 개수를 늘이면, 많은 엔트리를 가지고 있기는 하지만, 주변 데이터를 적게 가지고 있기 때문에 hit rate에 도움이 안 된다.   
따라서 이것을 적절하게 조절해야 한다. 결과적으로 종합적인 지식이 필요하다.   
SPEC2006 돌려본다.   
   
16k blocks : 캐시에 16000개의 라인 존재 = 16000개의 다른 데이터라는 의미.   
14bit to index block : 캐시에 addressing 할 수 있는 14bit.   
나머지는 tag bit.   
32-14-2(word offset)-2(byte offset)= 나머지 14bit는 tag bit이 된다. (offset을 제외하고)   
   
spatial locality의 또 하나의 장점 : 캐시 블럭마다 하나의 tag bit를 가짐. 한 캐시 블럭 내의 각 word들이 각각 tag bit을 가질 필요가 없다.   
따라서, 전체적으로 효율적인 효과를 가져온다는 이론이 존재한다.   
   
word offset : 한 캐시 블락 내에서도 몇 번째 word인지를 확인해야 한다. mux를 써야 한다는 것이다.   
byte offset :  char 읽었을 때 4개의 byte중 하나를 뽑기 위함.   
   
여기는 예시가 4 word이지만 요즘은 8,16 word를 쓰는 cache도 생기고 있다.   
점점 데이터의 양이 많아지고, SW자체가 커지고 있기 때문이다.   
    
L1은 레이싱카, L2는 승용차, L3는 SUV, DRAM은 Dump truck(속도는 느리지만, 데이터를 많이 가진다)    
   
- 9.10.1 Performance implications of increased blocksize   
blocksize가 늘어나면 cache의 total size(엔트리개수)가 줄어든다.   
처음에는 spatial locality 때문에 miss rate이 낮아지나, 어느순간부터는 miss rate가 높아진다.   
연구를 통해, inflection point를 발견한다.   
   
- 9.11 Flexible placement   
directed map은 캐시에 한 블록이 만들어지면, WS1에서 올 수 있고, WS2에서, WS3에서 올 수 있다.   
극단적이 예로 재수없게 cpu가 WS1,2,3만 계속 쓴다고하면, cache가 계속 miss가 난다.   
물론 L2로 넘어가면 몇배수로 크니까, hit이 되겠지만. L1은 계속 miss가 난다.(문제의 정의)   
그럼 Flexible placement는 뭘 하는 것인가?   
최근에 WS1을 읽어서 캐시에 저장했는데, directed-mapped 방법으로 인해 최근에 읽은 WS1은 최근 것임에도 불구하고 나가야 한다.   
이런 경우 temperal locality 가 희생받는다.   
Flexible placement은 최근에 쓴것을 지우면 안되니까, WS1,2,3 캐시 모두 올 수 있게 만든다. 다음번에 읽을 때는 캐시의 다음 블록에 저장한다.    
WS1,2,3 전부 다 캐시에 올 수 있다.(most flexible) 꽉 찼을 경우 가장 오래전에 있었던 데이터를 빼고 새로 가져온다.   
이로써 temperal locality를 극대화할 수 있다.   
locality가 캐시의 존재 이유였다.   
spatial locality은 direct-mapped에서 word 단위로 여러개 가져옴으로써 해결할 수 있었으나, temperal locality는 direct-mapped으로 가끔 해결할 수 없다.(예시와 같은 경우는 불가하다. 다른 때는 상관없다). 나쁜 것은 아니다.   
most flexible는 꽉 찬 경우 timestamp(캐시에 timestamp count를 저장하는 공간이 있다.)에서 가장 오래 전에 쓰여진 데이터를 뺀다.   
empty space가 있으면 빈 공간에 데이터를 넣는다. 따라서 temperal locality를 극대화해서 활용할 수 있다.   
그래서 Fully associative cache가 좋긴 하지만, 하드웨어가 무지하게 커진다.   
   
- 9.11.1 Fully associative cache   
메모리가 줄 서있지 않는다. 인덱스가 없다.   
tag를 통해 어느 자리든 갈 수 있다. 즉, 뒤죽박죽이 가능하다.   
valid bit가 1이고, tag bit가 같다면, data가 있다는 것이다. data는 mux로 뽑는다.   
most flexible할 수 있는 장점이 있다.   
그러나 단점은 tag bit 이 많이 늘었다.   
더 큰 문제는 directed-mapped은 hit을 알기 위해 comparator 1개 and-gate 1개가 필요했던 것에 반해,   
Fully associative cache 라인마다 다 가지고 있다. 16000개의 라인이 있다면, 각각 16000개씩 필요하다   
16000개를 or하는 mux. (tree 형식 만든다.) -> mux의 개수가 늘어난다.   
   
vtag : valid&tag   
   
- Two-way Set Associative   
directed를 2등분. 가로4개는 index로 비교하고, 세로 2개는 vtag로 비교한다. (절충안)    
엔트리 개수가 줄어들기 때문에 hit rate를 고려해야 한다. (fully는 hit rate으로부터 자유롭다. 하드웨어가 많아서 문제지만)   
이렇게 함으로써 fully associate cache보다 하드웨어를 줄일 수 있다.   
   
(page 48)four-way Set Associative    
hit rate은 낮아질 수 있지만, fully보다 하드웨어를 줄일 수있다.   
인덱스 하나에는 4개까지 넣을 수 있다. 4개 중에서 찾아내야 한다.   
최근에 하나를 읽었는데, 그 다음거 읽느랴고 방금 읽은 것을 지워버려야 하는 이슈가 경감이 됬다.(temperal locality)   
4번까지는 허용해준다. 2way는 2번까지, fully는 8번까지.   
comparator, and-gate의 개수가 줄어든다. tag bit도 조금 줄어든다.   
그러나 fully에 비해서 temperal locality를 완전히 해결하였다고 볼수 없다.   
   
4-way는 16000개의 데이터를 4로 나눈것이니 4000개의 엔트리 라인이 있는 것이다.    
   
여기서도 연구를 통해 최적을 조사한다.    
   
-----------------------------------------------------------------------------------------------   
- 9.11.3 Extremes of set associativity   
   
실제로는 다 쓰인다.   
cache layer 별로 어떤 것을 쓰는 것이 좋은 것인지 SPEC2006로 알아낸다.   
error free의 시스템을 만드는 것은 불가능하다.   
   
- 9.12 Instruction and Data caches   
캐시를 분리하는 것이 나을까?    
데이터, instruction은 분리하는 게 낫다.    
분리하는 구조인 하바드 아키텍처를 사용하는 것이 낫다.   
   
- 9.13 Reducing miss penalty   
miss rate을 줄이는게 중요. 이를 위해서는 캐시의 placement 중요, cache line의 크기, 그래서 spatial locality를 support하는 것이 중요.    
cache size를 늘리면 좋긴 하나, cache 동작 속도가 느려져 그 레벨에 캐시를 엑세스하는 시간이 늘어난다. 원하는 것 많으나 현실적인 문제.   
miss penaly 줄이는 방법   
L2, L3의 존재 이유는 miss penalty를 줄이기 위함.   
cache가 단계적으로 존재해서 penalty를 줄일 수 있다.   
상용적으로 보았을 때, 현재로써 반도체 공정이 cost effective하고, 성능 최적화한 방법이 L3까지이다.   
앞으로 반도체 공장의 발전으로 DRAM 과 processor의 gap이 좁혀져 L4가 생길 것이라 본다.   
   
cpu/memory performance gap   
cpu 의 성능은 매년 15~20%씩 증가하는데, memory는 10년에 33% 증가한다. 40년 전에는 캐시가 없던 시절이 있었다.   
gap이 커지면서 캐시가 들어옴. 점점 상대적으로 메인 메모리의 속도가 느려지니까 캐시로 보완하게 되었다.   
   
- 9.15 Recapping Types of Misses   
Compulsory : 프로그램을 처음 엑세스 할 떄(부팅할떄) 발생하는 강제적인 miss   
Capacity: 캐시의 사이즈가 제한되어있기 때문에, 오랜전에 있던 데이터를 찾지 못하였다.   
Conflict: 캐시는 full아닌데, line이 full.(direct map에서 발생) 캐시는 full 아닌데, 한 자리에서만 주고 빼고를 계속하여서 발생   
Fully associative는 conflict는 안 난다. 왜냐면, 비어있는 곳에 넣으면 되니까   
cache가 miss되어 사람이 느끼는 impact : Compulsory>Capacity>Conflict(부팅시>사람이 잘 느끼지 못함> 자주 발생하지 않는다)   
Compulsory를 해결하는 방법 : HDD -> SSD.   
Capacity는 잘 못느끼는 정도임. 물론 용량이 큰 프로그램을 돌릴 때는 자주 발생할 수 있다.    
   
- 9.16 Integrating TLB and Caches   
가상메모리의 전조   
cpu 다 0번지부터 시작된다. 그런데 컴퓨터의 물리적으로 0번지는 하난데 동시에 동작하는 SW는 많다.  빡침이 날 것임   
어떻게 동작하냐?   
가상의 세계에서 여러개의 SW는 분리되어 0번지부터 시작한다. 물리적인 0번지는 다르다.   
실시간으로 이것을 번역해줄수만 있으면 된다. TLB가 해준다.   
이 기능이 없으면 어떻게 되느냐?   
SW가 0번지가 아닌 각각 다른 특정 번지에서 시작해야 한다. 어디에서 시작해야하는지 알고있어야 한다.   
게다가 특정 SW를 사용하기 위해서는 모든 PC마다 SW가 사용하려는 번지를 남겨두어야 한다.   
   
- 9.19 Recap of Cache Design Considerations   
Direct mapped caches : 캐시가 temporal locality를 100% support 못해준다.   
Cache read/write algorithms : mapping 방법에 따라 hw 구성, placement, read/write 알고리즘이 달라진다.   
Spatial locality and blocksize : Spatial locality 때문에 cache line size(block size)를 늘린다.   
Fully- and set-associative caches : temporal locality를 support   
Considerations for I- and D-caches: structure hazard 때문에 분리해야 함   
TLB and caches :  가상주소에서 물리주소로 바꾸는 번역가. 엿보기하는 버퍼.     
Virtually indexed physically tagged caches : 학부수준으로는 힘들다.   
   
- 9.22 Performance implications of memory hierarchy   
cpu register는 latency가 거의 없다시피 한다.   
L1 : instruction queue가 없는 cpu는 3클락씩 안걸리고, 바로바로 register를 준다.   
IQ가 있으면, reorder하고 있어서 3클락만에 가져와도 된다. 아키텍쳐마다 다르다.   
   
9.23 Summary   
Degree of associativity : associativity 얼마나 할 것인가   
Cache access time : size에 제한을 받는다.   
LRU : 가장 오래전에 사용한 것을 뺀다.   
Simulated interleaving using DRAM : 여러개의 DRAM을 쓰게 될 경우, 스피드를 올리기 위해 DRAM을 번갈아쓰는 것    
   
9.24 Memory hierarchy of modern processors – An example   
temporal을 캐시의 아래 단계로 갈수록 강조함? 왜?   
fully associative 는 하드웨어가 잔뜩 들어간다. 그렇게 되면, 느려짐.   
캐시의 생명은 speed.   
L3은 어짜피 50 클락 필요한데, 53클락 필요하다고 하면 큰 차이가 있겠냐. 한 두 클락 더 써서 hit rate을 늘리겠다.   
   
L1과 L2는 분리를 하여 core마다 가지고 있다. L3는 어짜피 느리니까 공유한다.   
여기서도 연구를 통해 최적을 조사한다.    
    

## 7. 학습 내용에 대한 개인적인 총평
+ cache에 대한 내용 위주로 학습하였다.
+ cache line size에 따라 얻을 수 있는 이익과 손해에 대한 내용을 학습하였다.
+ cache를 사용하는 이유인 locality에 대한 내용과 여러 단계의 cache을 사용하는 이유인 miss penalty를 중점으로 확인하였다.
+ cache mapping 방법을 그림과 매칭할 수 있도록 이해하였다.
+ 내일은 VM 및 전범위 학습을 진행할 예정이다.

## 8. 다음 학습 계획
+ 컴퓨터구조 VM 및 전범위 학습