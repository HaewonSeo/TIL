## 1. 학습 날짜
+ 2020-11-05(목)

## 2. 학습시간
+ 12:00 ~ 18:00 (자가)   
+ 총 학습시간 : 6시간

## 3. 학습 범위 및 주제
+ 컴퓨터 구조 cpu multi-threading

## 4. 동료 학습 방법

## 5. 학습 목표
+ cpu multi-threading 학습

## 6. 상세 학습 내용
+ 실제 코딩에 소요한 시간 : 0시간    
+ 컴퓨터구조 강의 수강 : 4시간    

#### Multi-threading

* thread : 명령어의 흐름(상태를 가진), 하나의 독립된 프로그램
* 
* 어떻게 한개의 cpu에서 다중의 쓰레드를 돌릴것인가?
* 
* 결과적으로 멀티쓰레드로 hazard들을 모두 해결하고, 성능 증가의 핵심이 된다.
* 
* 최근 AMD가 인텔보다 가성비가 좋은 cpu를 만들게 된 이유가 AMD의 멀티쓰레드 기술의 발전 때문이다.
* 
* thread warping(in GPU. not in CPU) : 한 thread가 어디서 정체되면 휴지기처럼 보내버리고, 다른 thread를 실행한다. 사용자입장에서는 2개의 쓰레드가 동시에 돌아가는 듯한 느낌. 시간차를 이용해서 성능을 높이는. 이러한 아키텍쳐를 single instruction multithreading (sint)라는 기술이라 부름. 
* 
* 멀티쓰레딩을 잘하려면, 중간에 노는 시간을 다른 쓰레드로 채우는 것을 해야 함.
* 
* cpu자체를 멀티쓰레드 친화적으로 만들면, context switching 하는 시간을 줄일 수 있어서 시스템의 성능이 좋아진다.
* 
* 싱글 쓰레드의 성능을 낮출 수 있다. : A를 하다가 오래걸려서 그 사이에 B를 하는데 B를 하느랴고 A가 끝났다는 응답을 늦게 받는다.
* 
* >>Fine-grained 
* 여러 쓰레드가 있는데, 매클락마다 쓰레드를 바꾸는 것임.
* 쓰레드 한개 단위에서는 성능이 저조하다.
* 한 쓰레드가 논다고 해서 다른 쓰레드가 노는 것은 아니다. -> system-wise 성능은 증가
* data/control hazard를 해결한다. -> bubble도 사라짐
* 
* >단점
* ###중요###
* 정확히는 1/N로 내려가지는 않는다. 1/N보다는 덜 내려간다.
* 그 이유
* 1. 한 쓰레드가 200cycle논다고 해서, 1000cycle이 노는게 아니기 때문이다.
* 2. dependency checking과 branch prediction에서 발생하는 버블이 사라졌기 때문에 
* 그러나 전반적으로 1/N처럼 내려가는 것으로 보인다.
* 
* ##중요##
* 자원 집중이 증가 : 노는 쓰레드가 없어지고, 많이 사용하게 되었다. 소수의 하드웨어를 다중의 쓰레드들이 사용하려고 하는 확률이 높아진다. 즉, structure hazard의 확률은 증가할 수 있다.
* 
* 쓰레드간의 dependency를 체크해야 함
* 
* (수퍼 스케일라의 경우) 쓰레드 내의 여러 명령어들간의 디펜던시 체크, 브랜치 예측도 여천히 남아있는 문제.
* 
* (page10) 128개의 쓰레드, 150cycle동안 지연되어도 1번 노는 정도밖에 안된다.
* 
* >>Coares-grained
* fine은 자잘한것, coares는 큼직한것
* 큰 지연이 발생하면, 그때만 쓰레드를 바꾸고, 자잘자잘한 때는 바꾸지 않는다.
* 
* fine은 coares에 비해 구현이 쉽다.(매클락마다 바꾸는 정규화니까, 또 dependency checking과 branch prediction안해도 되니까(수퍼스케일라에서는 해야 하긴함))
* 그러나 coares는 큰 이벤트 발생 전까지 한 쓰레드 안에서 연산하니까 위 2가지를 해야한다.
* fine은 pc값과 레지스터 파일을 가지고 있다.
* 멀티쓰레드 친화적 - pc값 여러개, 레지스터도 여러개 가지고 있는 환경.
* coarse는 큰 이벤트 발생시 쓰레드 컨텍스 스위칭. 스택에 pc값과 레지스터를 올리고, 새로운 pc값과 레지스터 파일을 올리고 동작시키기 때문에. 오버헤드가 있다.
* 
* fine은 단일 스레드 성능이 낮다. 이 중에서도 우선순위가 높은 쓰레드가 늦게 처리된다는 문제 발생!
* 사용자 입장에서는 coarse가 낫다. 우선순위가 높은 쓰레드가 먼저 처리되니까 (체감성능상)
* 
* -------------------------------------------------------------
* SMT
* 
* >Simultaneous Multithreading
* 동시에 한다는 것이 핵심
* 명령어 처리개수를 엄청나게 늘려보자
* 다중에 싸이클에서 다중의 명령어를 한클락 사이클 안에서 봅아서 처리하자.
* 
* 멀티프로세스 -> 듀얼코어, 쿼드코어(물리적 배속)
* 멀티쓰레드 -> 한 프로세스 안에서 분리
* 
* 흰 네모는 버블
* 
* smt는 system-wise 성능이 극대화.
* 추가로 우선순위가 높은 쓰레드를 채울 수 있다. 즉, 싱글 쓰레드 성능도 거의 동일하게 맞춰줄 수 있다.
* 허점이 거의 없다.
* 문제는 구현의 어려움(hazard checking, multithreading)
* AMD 성공의 요인 :smt
* 
* >horizontal vs Verrical
* 파이프라인을 옆으로 누인것
* horizontal waste 
* verrical waste
* 중요하지 않다.
* 
* >smt
* 성능이 극대화(수평.수직 waste를 감소)
* 많은 양의 하드웨어 요구
* 
* >smt pipeline
* 명령어를 register mapping : 쓰레드마다 레지스터 데이터를 가지고 여기에 명령어를 매핑
* 명령어를 queue에 넣는다.
* for문에서 loop unrolling 하면 cpu(HW입장)는 훨씬 빠르다.(컴파일러는 loop unrolling 하면 exe크기가 굉장히 커짐(SW입장))
* map : 옛날 cpu에서 프로그램된 것을 지금 cpu에서 돌아가야 한다. 그것을 매핑한다.
* execute : 명령어를 다중처리할 수 있도록, 즉 수퍼스케일라
* retire :OoO이면, retire할 때 순서가 제대로 되었고, 문제가 없었는지 확인해야 함. 그것을 score board라고 함. 무슨 명령어가 어떻게 처리되었는지 정보를 가지고 있다가 끝나면 날려버린다. 앞의 작업에서 문제가 없었는지를 확인하는 단계 (성능에 영향을 주지는 않는다)
* 
* smt는 모두 뒤죽박죽이지만, sw엔지니어 입장에서는 뒤죽박죽처럼 보이지 않아야 한다.
* 
* smt를 하기위해서는?
* >replicated resource
* 쓰레드별로 자원이 필요. 다중 자원
* >shared resource
* 레지스터 파일은 share할수도 있고 안할수도 있는데, share하면 사이즈가 증가해서 쓰면 된다. 쓰레드마다 사용하는 레지스터 파일의 개수가 다를 경우 share하여 레지스터를 많이 필요로 하는 쓰레드가 많이 사용하면서 성능이 좋아진다.
* instruction queue : 공유해야 함. 큐는 하나
* caches : 캐시도 공유해야 함. 캐시를 따로 하면 하드웨어가 많이 필요함. 캐시는 비쌈. 반도체 면적의 1/3이 캐시. 
* translation lokkaside buffer(TLD): 나중에 배움, smt와 tld는 굉장히 중요
* branch predicator : 공유할수도 있고, 안할수도 있다. 스레드별로 달리 만들어도 된다.그러면 확률이 좋아진다. 브랜치 예측은 확률이 굉장히 중요하다. 브랜치 예측도 하드웨어 사이즈가 꽤 크다.
* 
* 녹색은 공유할 수 도 있고, 개별적으로 둘 수 도 있다,  설계자 마음. 많은 실험데이터로부터 근거를 만들어 결정
* 
* >Changes to OoO+Super Scalar for SMT. OoO와 SS를 하기 위한 변화, 여기에 SMT를 붙이면 극강. 
* 여러개의 pc중에서 하나를 선택하는 메카니즘이 필요하다.
* per-thread stack. 쓰레드마다 context가 다르기 때문
* 쓰레드별로 명령어의 흐름을 처리할 수 있는 메카니즘이 필요하다.
* 쓰레드 id별로 브랜치 타겟 버퍼를 만들어야 한다. 
* 큰 레지스터 파일이 필요하다. 
* 
* 보통 논문 발표 5년 이후 상용화가 된다.
* 
* >SMT Scalability
* 한 사이클에 4개(max)의 명령어를 가져올 수 있는데, 싱글 쓰레드일 경우 평균적으로 2.1개 정도 가져온다.
* 쓰레드 증가할 수록 4개에 근접해지고, bound된다.
* 쓰레드 개수를 늘릴때마다 cpu HW size는 꽤 많이 증가한다. 쓰레드별로 검사해주는 하드웨어의 증가(n배 증가)
* 보통 쓰레드 3~4개로 선택한다.(HW를 고려한 최적의 개수를 결정한다, 실험을 통해 효율성을 결정)
* 
* ---------------------------------------------------------------
* multithreading 3
* 
* >SMT design conssideration
* 1.매클락마다 어느 쓰레드에서 명령어를 가져오는데 우선순위를 어떻게 할 것인가.
* 
* 2.공유 자원 할당 정책
* -어떻게 stavation(처리할 명령어가 없어서 파이프라인이 놀고 있는 것)을 방지할지
* - 처리율을 최대화할 수 있는지
* -어느 쓰레드에서 명령어를 가져오는 것이 공평한지, 전체 쓰레드 처리의 Quality를 어떻게 유지할지
* -모두한테 하드웨어 자원을 막 줄 것인지, 구분되어 줄건지
* 
* 
* >1. SMT Fetch Policies(1)
* - static : 정적인. 하나로 고정한 체계
*  - round-robin : 매 클락마다 하나씩 받아오자 단순.
* 
* - dynamic 
*  - 브랜치가 적은 쓰레드를 선호(브랜치 예측도 해야하하고 브랜치 많으면 flush 문제가 많다) 
*  - outstanding misses : 캐시미스가 적은 스레드를 선호 (캐시 미스 : 빠른 동작을 하는 캐시한테 명령어를 달라고 했는데, 캐시에 없어. 그러면 ddr 메모리로 가야하는데 메모리는 200cycle정도를 손해본다. 그동안 cpu는 기다려야 한다.) -> cpu 파이프라인 stage fill rate을 최대화 하겠다.
*  - 명령어의 수가 적은 쓰레드를 선호(쓰레드마다 명령어의 개수가 다르다)
*  - real time requirments : 실시간 연산성을 요청하는 쓰레드 (이메일 확인여부보다 사용자가 현재 사용중인 마우스 반응이 더 중요), 사용자가 현재 사용중인 쓰레드를 선호. hard real-time같은 경우는 통신의 경우인데, 송신이 된지 0.3초 안에 받아야 수신해야 하는. 수신하지 않으면 신호를 놓치게 된다.
* 이는 모든 경우를 고려해야 하기 때문에 구현하기 쉽지 않다. 
* 
* => round-robin은 좋지 않다. 느린 쓰레드가 파이프라인을 점유하게 되어 전체 성능이 느려지게 된다. monopoly(매점매석)
* 
* >1. SMT Fetch Policies(2)
* Icount : ealry stage에서 명령어의 개수가 적은 쓰레드를 가져온다. 빠르게 처리되는 스레드를 빨리 처리해주자.
* 
* RR 2.8 : round-robin에서 명령어를 한번에 8개를 가져올 수 있는 cpu에서 스레드당 2개씩 가져온다. 이게 가장 성능이 좋다(그래프상에서)
* Icount 2.8 이 가장 성능이 좋다
* --> SMT 안에서도 fetch policy를 바꾸니까 성능이 좋다.
* 
* 논문 : <people.csail.mit.edu/emer/papers/1996.05.isca.smt.pdf>
* (논문11page Icount부분)
* 
* Icount : 파이프라인 앞단에 있는 스테이지에서 명령어를 가장 적게 가지고 있는 쓰레드를 favor주자; 3가지 purpose(목적)
* (1)IQ(instructuon queue,전체 stage중 3번재 stage): 한쓰레드가 IQ를 점유해버리는일을 막는다.(page15를 막는것)
* (2) IQ를 가장 효과적으로 passing하는 스레드 (dependency issue가 없어 빠르게 처리되는 스레드들) 빈공간을 잘 채워주면, 시스템 성능이 높아진다. 
* 즉, icount로 명령어가 적은 스레드가 IQ에 들어오고, IQ에 들어온 명령어가 빨리 처리될 수록 성능이 증가한다.
* (3) 처리가능한 스레드들로부터의 더 합리적인 명령어 분배가 될 것이다. queue에서의 병렬처리는 최대화하면서 
* 
* queue에서 OoO를 한다. 순서를 바꾼다.
* fetch decode map는 명령어를 처리하기 위한 준비단계임
* 그 이후에 명령어를 처리한다.
* 
* 인텔은 지금도 hyper-threading 하고 있다.
* hyperthreading : 요즘은 보통 threading이 core의 두배. SMT를 해준다. 요즘 상용 cpu는 supersclar를 2개로 해준다(효율적이니까). hyperthreading이 안되면, SMT를 안하는 것이고 core와 threading이 같다.
* 
* (page 33) : 15stage
* 요즘은 아마 20stage까지 한다. 
* trace cache : 원래 캐시는 최근에 썼던 명령어를 다시 쓰거나, 근처에 있는 명령어을 처리하는 것인데, trace cache는 최근에 처리한 순서대로 명령어를 쌓아 놓는 것임. trace cache는 최근에 브랜치한 패턴을 저장해놓는다.
*   
* hyper-threading 을 하기 때문에 rename이 필요하다.(수퍼스케일라니까)
* sched 까지가 준비단계
* 실제로 쓰는 기술이다.
* 
* 상용 cpu, server cpu, amd cpu도 보통 2개 쓰레드를 한다. 그 이상은 하드웨어 구현이 너무 어려워진다. 효율적이지 않다.
* 스레드 2개만 돌려줘도 성능이 많이 좋아진다.


## 7. 학습 내용에 대한 개인적인 총평
+ cpu의 발전이 이루어지기까지의 기술들의 변화 과정들을 배웠다.
+ 학부생은 답이 있는 문제, 석사/박사는 답이 없는 문제를 해결한다. 
+ 엔지니어란? 답이 없는 문제를 연구를 통해 답을 내는 것. 기술을 이용하는 사람이 될 것인가, 기술을 만드는 테크니션이 될 것인가.
+ 연구자는 기대하는 결과를 가지고 실험을 통해 기대값을 얻어낸다. 연구는 답이 없는 문제에 대한 끊임없는 고민을 한다.
+ 능력은 한 번 생기면, 없어지지 않는다. 반드시 사용할 일이 생긴다. 능력을 키우는 것이 중요하지, 이것을 어떻게 쓸지 고민하며 시간을 보내는 것은 중요하지 않다.

## 8. 다음 학습 계획
+ 소프트웨어공학 문제기술서 및 UML-V프로세스 6단계 작성
+ 소프트웨어공학 설계 강의 수강